{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae07cfa5",
   "metadata": {},
   "source": [
    "# Naive Approach:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debb4ecb",
   "metadata": {},
   "source": [
    "# 1. What is the Naive Approach in machine learning?\n",
    "\n",
    "A naive classifier model is one that does not use any sophistication in order to make a prediction, typically making a random or constant prediction. Such models are naive because they don't use any knowledge about the domain or any learning in order to make a prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325851bd",
   "metadata": {},
   "source": [
    "#  2. Explain the assumptions of feature independence in the Naive Approach\n",
    "\n",
    "The naive Bayes classifier assumes that all features in the input data are independent of each other, which is often not true in real-world scenarios. However, despite this simplifying assumption, the naive Bayes classifier is widely used because of its efficiency and good performance in many real-world applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7587ffda",
   "metadata": {},
   "source": [
    "# 3. How does the Naive Approach handle missing values in the data?\n",
    "\n",
    "Naïve Bayes Imputation (NBI) is used to fill in missing values by replacing the attribute information according to the probability estimate. The NBI process divides the whole data into two sub-sets is the complete data and data containing missing data. Complete data is used for the imputation process at the lost value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba1e1c2",
   "metadata": {},
   "source": [
    "# 4. What are the advantages and disadvantages of the Naive Approach?\n",
    "\n",
    "The advantage is that it is inexpensive to develop, store data, and operate. The disadvantage is that it does not consider any possible causal relationships that underly the forecasted variable. This model adds the latest observed absolute period -to-period change to the most recent observed level of the variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b530189d",
   "metadata": {},
   "source": [
    "# 5. Can the Naive Approach be used for regression problems? If yes, how?\n",
    "\n",
    "Naive Bayes is a supervised classification algorithm that is used primarily for dealing with binary and multi-class classification problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcdc255",
   "metadata": {},
   "source": [
    "# 6. How do you handle categorical features in the Naive Approach?\n",
    "\n",
    "however naive algorithm doesn't handle missing values itself but we can use several missing value imputations technic to resolve the issue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93396845",
   "metadata": {},
   "source": [
    "# 7. What is Laplace smoothing and why is it used in the Naive Approach?\n",
    "\n",
    "Laplace smoothing is a smoothing technique that helps tackle the problem of zero probability in the Naïve Bayes machine learning algorithm. Using higher alpha values will push the likelihood towards a value of 0.5, i.e., the probability of a word equal to 0.5 for both the positive and negative reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19519f5",
   "metadata": {},
   "source": [
    "# 8. How do you choose the appropriate probability threshold in the Naive Approach?\n",
    "\n",
    "The Naive Bayes algorithm is one of the most popular and simple machine learning classification algorithms. It is based on the Bayes' Theorem for calculating probabilities and conditional probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851d5177",
   "metadata": {},
   "source": [
    "# 9. Give an example scenario where the Naive Approach can be applied\n",
    "\n",
    "Some best examples of the Naive Bayes Algorithm are sentimental analysis, classifying new articles, and spam filtration. Classification algorithms are used for categorizing new observations into predefined classes for the uninitiated data. The Naive Bayes Algorithm is known for its simplicity and effectiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499af8c7",
   "metadata": {},
   "source": [
    "# KNN:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04301617",
   "metadata": {},
   "source": [
    "# 10. What is the K-Nearest Neighbors (KNN) algorithm?\n",
    "\n",
    "The k-nearest neighbors algorithm, also known as KNN or k-NN, is a non-parametric, supervised learning classifier, which uses proximity to make classifications or predictions about the grouping of an individual data point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a836b436",
   "metadata": {},
   "source": [
    "# 11. How does the KNN algorithm work?\n",
    "\n",
    "KNN works by finding the distances between a query and all the examples in the data, selecting the specified number examples (K) closest to the query, then votes for the most frequent label (in the case of classification) or averages the labels (in the case of regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417f509c",
   "metadata": {},
   "source": [
    "# 12. How do you choose the value of K in KNN?\n",
    "\n",
    "The optimal K value usually found is the square root of N, where N is the total number of samples. Use an error plot or accuracy plot to find the most favorable K value. KNN performs well with multi-label classes, but you must be aware of the outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab3484b",
   "metadata": {},
   "source": [
    "# 13. What are the advantages and disadvantages of the KNN algorithm?\n",
    "\n",
    "Advantages - \n",
    "\n",
    "Simple and Easy to Understand\n",
    "\n",
    "Non-parametric\n",
    "\n",
    "Can Handle Large Datasets\n",
    "\n",
    "\n",
    "Disadvantages - \n",
    "\n",
    "Sensitive to Outliers\n",
    "\n",
    "Computationally Expensive\n",
    "\n",
    "Requires Good Choice of K\n",
    "\n",
    "Limited to Euclidean Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674ec182",
   "metadata": {},
   "source": [
    "# 14. How does the choice of distance metric affect the performance of KNN?\n",
    "\n",
    "In addition, the performance of the KNN with this top performing distance degraded only ∼20% while the noise level reaches 90%, this is true for most of the distances used as well. This means that the KNN classifier using any of the top 10 distances tolerates noise to a certain degree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13117ab6",
   "metadata": {},
   "source": [
    "# 15. Can KNN handle imbalanced datasets? If yes, how?\n",
    "\n",
    "The KNN algorithm is impacted by the imbalanced data and it becomes biased towards the more frequent output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2342a80e",
   "metadata": {},
   "source": [
    "# 16. How do you handle categorical features in KNN\n",
    "\n",
    "Among the three classification methods, only Kernel Density Classification can handle the categorical variables in theory, while kNN and SVM are unable to be applied directly since they are based on the Euclidean distances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c10678",
   "metadata": {},
   "source": [
    "# 17. What are some techniques for improving the efficiency of KNN?\n",
    "\n",
    "In order to improve the efficiency and speed of KNN, reducing the dimensionality of the data is necessary. The curse of dimensionality can make the distance between data points less distinguishable and increase complexity for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a683672e",
   "metadata": {},
   "source": [
    "# 18. Give an example scenario where KNN can be applied.\n",
    "\n",
    "Speech Recognition, Handwriting Detection, Image Recognition and Video Recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fc5709",
   "metadata": {},
   "source": [
    "# Clustering:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ede65c",
   "metadata": {},
   "source": [
    "# 19. What is clustering in machine learning?\n",
    "\n",
    "Clustering is the task of dividing the population or data points into a number of groups such that data points in the same groups are more similar to other data points in the same group and dissimilar to the data points in other groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1b3a8d",
   "metadata": {},
   "source": [
    "# 20. Explain the difference between hierarchical clustering and k-means clustering\n",
    "\n",
    "k-means is method of cluster analysis using a pre-specified no. of clusters. It requires advance knowledge of 'K'. Hierarchical clustering also known as hierarchical cluster analysis (HCA) is also a method of cluster analysis which seeks to build a hierarchy of clusters without having fixed number of cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a210be",
   "metadata": {},
   "source": [
    "# 21. How do you determine the optimal number of clusters in k-means clustering\n",
    "\n",
    "The silhouette coefficient may provide a more objective means to determine the optimal number of clusters. This is done by simply calculating the silhouette coefficient over a range of k, & identifying the peak as optimum K."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb2cd1f",
   "metadata": {},
   "source": [
    "# 22. What are some common distance metrics used in clustering?\n",
    "\n",
    "The four types of distance metrics are Euclidean Distance, Manhattan Distance, Minkowski Distance, and Hamming Distance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effd740c",
   "metadata": {},
   "source": [
    "# 23. How do you handle categorical features in clustering?\n",
    "\n",
    "Unlike Hierarchical clustering methods, we need to upfront specify the K.\n",
    "Pick K observations at random and use them as leaders/clusters.\n",
    "Calculate the dissimilarities and assign each observation to its closest cluster.\n",
    "Define new modes for the clusters.\n",
    "Repeat 2–3 steps until there are is no re-assignment required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cd6b30",
   "metadata": {},
   "source": [
    "# 24. What are the advantages and disadvantages of hierarchical clustering?\n",
    "\n",
    "The advantage of Hierarchical Clustering is we don't have to pre-specify the clusters. However, it doesn't work very well on vast amounts of data or huge datasets. And there are some disadvantages of the Hierarchical Clustering algorithm that it is not suitable for large datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984adf5b",
   "metadata": {},
   "source": [
    "# 25. Explain the concept of silhouette score and its interpretation in clustering.\n",
    "\n",
    "The silhouette value is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The silhouette ranges from −1 to +1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32d4671",
   "metadata": {},
   "source": [
    "# 26. Give an example scenario where clustering can be applied.\n",
    "\n",
    "clustering can be used to identify several types of depression. Cluster analysis is also used to identify patterns in the spatial or temporal allocation of a disease. Business − Businesses collect huge amounts of data on current and potential users."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6445f0",
   "metadata": {},
   "source": [
    "# Anomaly Detection:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afebaf29",
   "metadata": {},
   "source": [
    "# 27. What is anomaly detection in machine learning?\n",
    "\n",
    "Anomaly detection is the identification of rare events, items, or observations which are suspicious because they differ significantly from standard behaviors or patterns. Anomalies in data are also called standard deviations, outliers, noise, novelties, and exceptions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ea7e92",
   "metadata": {},
   "source": [
    "# 28. Explain the difference between supervised and unsupervised anomaly detection.\n",
    "\n",
    "Supervised learning harnesses the power of labeled data to train models that can make accurate predictions or classifications. In contrast, unsupervised learning focuses on uncovering hidden patterns and structures within unlabeled data, using techniques like clustering or anomaly detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ace257",
   "metadata": {},
   "source": [
    "# 29. What are some common techniques used for anomaly detection\n",
    "\n",
    "Statistical (Z-score, Tukey's range test and Grubbs's test)\n",
    "Density-based techniques (k-nearest neighbor, local outlier factor, isolation forests, and many more variations of this concept)\n",
    "Subspace-, correlation-based and tensor-based outlier detection for high-dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04d5f78",
   "metadata": {},
   "source": [
    "# 30. How does the One-Class SVM algorithm work for anomaly detection?\n",
    "\n",
    "The one-class SVM finds a hyper-plane that separates the given dataset from the **origin** such that the hyperplane is as close to the datapoints as possible. Note that usually the RBF kernel is used to fit a non-linear boundary around the dense region of the dataset separating the remaining points as outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3448ec50",
   "metadata": {},
   "source": [
    "# 31. How do you choose the appropriate threshold for anomaly detection?\n",
    "\n",
    "In the Upper Limit and Lower Limit fields, specify the upper and lower threshold limits.\n",
    "In the Consecutive Occurrences spinner, specify the number of consecutive threshold violations that must occur before an anomaly and an event is generated.\n",
    "In the Type drop-down list, specify the threshold type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51b9aa2",
   "metadata": {},
   "source": [
    "# 32. How do you handle imbalanced datasets in anomaly detection?\n",
    "\n",
    "Under-sampling. Under-sampling balances the dataset by reducing the size of the abundant class.\n",
    "Over-sampling. On the contrary, oversampling is used when the quantity of data is insufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5013f5",
   "metadata": {},
   "source": [
    "# 33. Give an example scenario where anomaly detection can be applied.\n",
    "\n",
    " a credit card company will use anomaly detection to track how customers typically use their credit cards. If a customer makes an abnormally large purchase or a purchase in a new location, the algorithm recognizes the anomaly and alerts a team member to contact the customer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16af866",
   "metadata": {},
   "source": [
    "# Dimension Reduction:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9de8ca7",
   "metadata": {},
   "source": [
    "# 34. What is dimension reduction in machine learning?\n",
    "\n",
    "Dimensionality reduction is the process of reducing the number of features in a dataset while retaining as much information as possible. This can be done to reduce the complexity of a model, improve the performance of a learning algorithm, or make it easier to visualize the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ef8221",
   "metadata": {},
   "source": [
    "# 35. Explain the difference between feature selection and feature extraction\n",
    "\n",
    "The key difference between feature selection and feature extraction techniques used for dimensionality reduction is that while the original features are maintained in the case of feature selection algorithms, the feature extraction algorithms transform the data onto a new feature space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadb4b0e",
   "metadata": {},
   "source": [
    "# 36. How does Principal Component Analysis (PCA) work for dimension reduction?\n",
    "\n",
    "Principal component analysis, or PCA, is a dimensionality reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8799d780",
   "metadata": {},
   "source": [
    "# 37. How do you choose the number of components in PCA?\n",
    "\n",
    "The common way of selecting the Principal Components to be used is to set a threshold of explained variance, such as 80%, and then select the number of components that generate a cumulative sum of explained variance as close as possible of that threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7df7e5",
   "metadata": {},
   "source": [
    "# 38. What are some other dimension reduction techniques besides PCA?\n",
    "\n",
    "There are several techniques for dimensionality reduction, including principal component analysis (PCA), singular value decomposition (SVD), and linear discriminant analysis (LDA). Each technique uses a different method to project the data onto a lower-dimensional space while preserving important information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b47275",
   "metadata": {},
   "source": [
    "# 39. Give an example scenario where dimension reduction can be applied.\n",
    "\n",
    "Dimensionality reduction is common in fields that deal with large numbers of observations and/or large numbers of variables, such as signal processing, speech recognition, neuroinformatics, and bioinformatics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5cf22f",
   "metadata": {},
   "source": [
    "# Feature Selection:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c004cd8e",
   "metadata": {},
   "source": [
    "# 40. What is feature selection in machine learning?\n",
    "\n",
    "Feature selection is used to choose a subset of relevant features for effective classification of data. In high dimensional data classification, the performance of a classifier often depends on the feature subset used for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db759ab",
   "metadata": {},
   "source": [
    "# 41. Explain the difference between filter, wrapper, and embedded methods of feature selection.\n",
    "\n",
    "Filter methods perform the feature selection independently of construction of the classification model. Wrapper methods iteratively select or eliminate a set of features using the prediction accuracy of the classification model. In embedded methods the feature selection is an integral part of the classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bae1aa",
   "metadata": {},
   "source": [
    "# 42. How does correlation-based feature selection work?\n",
    "\n",
    "Features with high correlation are more linearly dependent and hence have almost the same effect on the dependent variable. So, when two features have high correlation, we can drop one of the two features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cc8b00",
   "metadata": {},
   "source": [
    "# 43. How do you handle multicollinearity in feature selection?\n",
    "\n",
    "Removing variables. A straightforward method of correcting multicollinearity is removing one or more variables showing a high correlation. ...\n",
    "More data. ...\n",
    "Using techniques such as partial least squares regression (PLS) and principal component analysis (PCA). ...\n",
    "Centering the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe79fbb",
   "metadata": {},
   "source": [
    "# 44. What are some common feature selection metrics?\n",
    "\n",
    "There are three types of feature selection: Wrapper methods (forward, backward, and stepwise selection), Filter methods (ANOVA, Pearson correlation, variance thresholding), and Embedded methods (Lasso, Ridge, Decision Tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a894106d",
   "metadata": {},
   "source": [
    "# 45. Give an example scenario where feature selection can be applied\n",
    " some real-life examples of feature selection: Mammographic image analysis. Criminal behavior modeling. Genomic data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83b78d5",
   "metadata": {},
   "source": [
    "# Data Drift Detection:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979e25e2",
   "metadata": {},
   "source": [
    "# 46. What is data drift in machine learning?\n",
    "\n",
    "Data drift, also known as covariate shift, occurs when the distribution of the input data changes over time. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc055f63",
   "metadata": {},
   "source": [
    "# 47. Why is data drift detection important?\n",
    "\n",
    "Data drift is an important concept in machine learning because it can cause a trained model to become less accurate over time, leading to incorrect predictions and suboptimal decision-making."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fac755",
   "metadata": {},
   "source": [
    "# 48. Explain the difference between concept drift and feature drift.\n",
    "\n",
    "Concept Drift in machine learning is a situation where the statistical properties of the target variable (what the model is trying to predict) change over time.\n",
    "\n",
    "Feature drift occurs when there are changes in the distribution of a model's inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90935ae",
   "metadata": {},
   "source": [
    "# 49. What are some techniques used for detecting data drift?\n",
    "\n",
    "Time distribution-based methods use statistical methods to calculate the difference between two probability distributions to detect drift. These methods include the Population Stability Index, KL Divergence, JS Divergence, KS Test, and the Wasserstein Metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3ef7b0",
   "metadata": {},
   "source": [
    "# 50. How can you handle data drift in a machine learning model?\n",
    "\n",
    "Some strategies for addressing drift include continuously monitoring and evaluating the performance of a model, updating the model with new data, and using machine learning models that are more robust to drift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f26ec4",
   "metadata": {},
   "source": [
    "# Data Leakage:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2622b39d",
   "metadata": {},
   "source": [
    "# 51. What is data leakage in machine learning\n",
    "\n",
    "data leakage happens when the training data contains information about the target, but similar data won't be available when the model is used for prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86229a65",
   "metadata": {},
   "source": [
    "# 52. Why is data leakage a concern?\n",
    "\n",
    "data leakage leads to high performance on the training dataset (and possibly even the validation data), but the model will perform badly in production."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ec5333",
   "metadata": {},
   "source": [
    "# 53. Explain the difference between target leakage and train-test contamination.\n",
    "\n",
    "Target leakage can happen when a variable that is not a feature is being used to predict the target. Target leakage happens more frequently with complex datasets.\n",
    "\n",
    "train-test contamination happens when we unknowingly or subtly pass information from our train dataset to our validation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880c79b1",
   "metadata": {},
   "source": [
    "# 54. How can you identify and prevent data leakage in a machine learning pipeline?\n",
    "\n",
    "To prevent such leakage, the dataset should be separated into training and testing sets before performing any preprocessing steps. The preprocessing steps should only be fit on the training dataset and then applied to both the training and test datasets separately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f291e8fa",
   "metadata": {},
   "source": [
    "# 55. What are some common sources of data leakage?\n",
    "\n",
    "A data leak is when information is exposed to unauthorized people due to internal errors. This is often caused by poor data security and sanitization, outdated systems, or a lack of employee training. Data leaks could lead to identity theft, data breaches, or ransomware installation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889f3cd8",
   "metadata": {},
   "source": [
    "# 56. Give an example scenario where data leakage can occur.\n",
    "\n",
    "Data exposed in transit — Data transmitted via emails, API calls, chat rooms, and other communications. Data exposed at rest — Can occur due to misconfigured cloud storage, insecure databases, or unattended or lost devices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02dc9cc",
   "metadata": {},
   "source": [
    "# Cross Validation:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d688b08c",
   "metadata": {},
   "source": [
    "# 57. What is cross-validation in machine learning?\n",
    "\n",
    "Cross validation is a technique used in machine learning to evaluate the performance of a model on unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bd238f",
   "metadata": {},
   "source": [
    "# 58. Why is cross-validation important?\n",
    "\n",
    "The purpose of cross–validation is to test the ability of a machine learning model to predict new data. It is also used to flag problems like overfitting or selection bias and gives insights on how the model will generalize to an independent dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dda4cc",
   "metadata": {},
   "source": [
    "# 59. Explain the difference between k-fold cross-validation and stratified k-fold cross-validation.\n",
    "\n",
    "KFold devides the dataset into k folds. Where as Stratified ensures that each fold of dataset has the same proportion of observations with a given label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3da2b44",
   "metadata": {},
   "source": [
    "# 60. How do you interpret the cross-validation results?\n",
    "\n",
    "Cross validation is a technique that allows us to produce test set like scoring metrics using the training set. That is, it allows us to simulate the effects of “going out of sample” using just our training data, so we can get a sense of how well our model generalizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e29bb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
